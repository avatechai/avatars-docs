---
title: "Avatar Interaction"
---

Avatars can interact with several ways:

## Lip-sync

The avatars generated from [AvatarLabs](https://labs.avatech.ai) support real-time lip-sync based on audio provided to the SDK.

To make your avatar start talking, you need to create an `AudioContext` and connect it to the SDK with `connectAudioContext`.

```jsx
function App(){
    const {
        avatarDisplay,
        connectAudioElement,
        connectAudioContext,
        connectAudioNode
    } = useAvatar({
        //...
    });

    const audioContextRef = useRef<AudioContext>(new AudioContext());

    return (
        {avatarDisplay}
        <button
            onClick={() => {
              connectAudioContext(audioContextRef.current)
            }}
        >
            Connect AudioContext
        </button>
    );
}
```



### Connect Audio

After connecting AudioContext, you need to create a HTMLAudioElement or AudioNode and connect it to the SDK in order to sync the audio with lip-sync system.

<Tabs>
  <Tab title="HTMLAudioElement">

After connection, when you play the audio from the HTMLAudioElement, the avatar will start lip-sync automatically.

```jsx
const { avatarDisplay, context, connectAudioElement } = useAvatar({
  //...
});

const audioPlayer = useRef(new Audio())

//...

return (
    {avatarDisplay}
    <button
        onClick={() => {
          connectAudioElement(audioPlayer.current, audioContextRef.current);
        }}
    >
        Connect 
    </button>

    <button
        onClick={() => {
          audioPlayer.current.src = "audio.mp3";
          audioPlayer.current.play();
        }}
    >
        Play
    </button>
)
```
  </Tab>
  <Tab title="AudioNode">

    You need to connect the AudioNode to the SDK everytime a new audio file is uploaded.

```jsx
const { avatarDisplay, context, connectAudioNode } = useAvatar({
  //...
});

//...

return (
    {avatarDisplay}

    // upload audio file
    <input
        type="file"
        onChange={(e) => {
          setFile(e.target.files![0]);
        }}
      ></input>

    // play audio from file
    <button
        onClick={() => {
          const nf = await file!.arrayBuffer();

          const audioBuffer = await audioContextRef.current.decodeAudioData(nf);
          const bs = audioContextRef.current.createBufferSource();
          bs.buffer = audioBuffer;

          connectAudioNode(bs);
          bs.start();
        }}
    >
        Play
    </button>
)
```


  </Tab>
</Tabs>

<Info>
  You can visit [here](./guides/nextjs-demo) for a complete demo shocasing how
  our lip-sync system works!
</Info>

## Expression

Avatars created from our creation suite support different expressions.

<Info>
  How many expression the avatar can make depends on which model you are using
  to generate the avatar.
</Info>

```jsx
const { avatarDisplay, availableEmotions, setEmotion } = useAvatar({
  //...
});
return (
  <button
    onClick={() => {
      setEmotion(availableEmotions[0]);
    }}
  >
    Set emotion
  </button>
);
```
